{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer: Gridsearches are run using 50 CPU threads, so don't run unless you want to wait eons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build our models to accurately determine if a post is from TheSilphRoad or pokemongo subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting, enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from googletrans import Translator, LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>merged_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheSilphRoad</td>\n",
       "      <td>Fix to not being able to attack? Has anybody f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheSilphRoad</td>\n",
       "      <td>Attack glitch during Regi raids 2 raids today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheSilphRoad</td>\n",
       "      <td>[Bug?] Can’t seem to earn or collect pokecoins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheSilphRoad</td>\n",
       "      <td>[Bug?] AR suddenly freezes Using an iPhone 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheSilphRoad</td>\n",
       "      <td>3 hour incense event personal results For any ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                        merged_text\n",
       "0  TheSilphRoad  Fix to not being able to attack? Has anybody f...\n",
       "1  TheSilphRoad  Attack glitch during Regi raids 2 raids today ...\n",
       "2  TheSilphRoad  [Bug?] Can’t seem to earn or collect pokecoins...\n",
       "3  TheSilphRoad  [Bug?] AR suddenly freezes Using an iPhone 11,...\n",
       "4  TheSilphRoad  3 hour incense event personal results For any ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "\n",
    "merged_df = pd.read_csv('../data/merged.csv')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>merged_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fix to not being able to attack? Has anybody f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Attack glitch during Regi raids 2 raids today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Bug?] Can’t seem to earn or collect pokecoins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[Bug?] AR suddenly freezes Using an iPhone 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3 hour incense event personal results For any ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                        merged_text\n",
       "0          1  Fix to not being able to attack? Has anybody f...\n",
       "1          1  Attack glitch during Regi raids 2 raids today ...\n",
       "2          1  [Bug?] Can’t seem to earn or collect pokecoins...\n",
       "3          1  [Bug?] AR suddenly freezes Using an iPhone 11,...\n",
       "4          1  3 hour incense event personal results For any ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set TheSilphRoad to 1 and pokemongo to 0\n",
    "merged_df['subreddit'] = np.where(merged_df['subreddit'] == 'TheSilphRoad', 1, 0)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit      0\n",
       "merged_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.dropna(inplace = True)\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19254, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom stop words list to remove all similar words we found in the previous notebook\n",
    "\n",
    "# start with the base english stopwords\n",
    "new_stopwords = stopwords.words('english')\n",
    "\n",
    "# add stopwords that will easily identify a silph post\n",
    "# also added stopwords that are common across both subreddits and stop words as a result and lemmatizing and stemming\n",
    "custom_words = ['silph', 'road', 'silphroad', 'thesilphroad', 'pokemon', 'go', 'get', 'one', 'like', 'would', 'know', 'time', 'game', 'shiny', \n",
    "               'https', 'raid', 'anyone', 'got', 'new', 'event', 'day', 'level', 'even', 'com', 'raids', 'still', 'people', 'also', 'since',\n",
    "               'use', 'catch', 'amp', 'see', 'want', 'could', 'first', 'research', 'shadow', 'think', 'else', 'way', 'niantic', 'make', \n",
    "               'back', 'really', 'need', 'eggs', 'community', 'something', 'much', 'good', 'able', \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'abl', \n",
    "                'abov', 'ani', 'anyon', 'becaus', 'befor', 'commun', 'doe', 'dure', 'egg', 'els', 'ha', 'hi', 'http', 'might', 'must', \n",
    "                \"n't\", 'onc', 'onli', 'ourselv', 'peopl', 'realli', 'sha', 'shini', 'sinc', 'someth', 'themselv', 'thi', 'veri', 'wa', \n",
    "                'whi', 'wo', 'yourselv', 'becau', 'el']\n",
    "\n",
    "new_stopwords.extend(custom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000000003</th>\n",
       "      <th>0000000001</th>\n",
       "      <th>0000006</th>\n",
       "      <th>000002322</th>\n",
       "      <th>000010</th>\n",
       "      <th>0004</th>\n",
       "      <th>000exp</th>\n",
       "      <th>...</th>\n",
       "      <th>что</th>\n",
       "      <th>шикарно</th>\n",
       "      <th>это</th>\n",
       "      <th>から毎週金曜よる6時55分にお引っ越しすることを記念し</th>\n",
       "      <th>そのうち10月6日</th>\n",
       "      <th>シャトウ</th>\n",
       "      <th>ツ_</th>\n",
       "      <th>テレヒ東京系にて放送中のアニメ</th>\n",
       "      <th>ホケットモンスター</th>\n",
       "      <th>特別な内容て開催されます</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000000003  0000000001  0000006  000002322  000010  0004  \\\n",
       "0   0    0     0            0           0        0          0       0     0   \n",
       "1   0    0     0            0           0        0          0       0     0   \n",
       "2   0    0     0            0           0        0          0       0     0   \n",
       "3   0    0     0            0           0        0          0       0     0   \n",
       "4   0    0     0            0           0        0          0       0     0   \n",
       "\n",
       "   000exp  ...  что  шикарно  это  から毎週金曜よる6時55分にお引っ越しすることを記念し  そのうち10月6日  \\\n",
       "0       0  ...    0        0    0                            0          0   \n",
       "1       0  ...    0        0    0                            0          0   \n",
       "2       0  ...    0        0    0                            0          0   \n",
       "3       0  ...    0        0    0                            0          0   \n",
       "4       0  ...    0        0    0                            0          0   \n",
       "\n",
       "   シャトウ  ツ_  テレヒ東京系にて放送中のアニメ  ホケットモンスター  特別な内容て開催されます  \n",
       "0     0   0                0          0             0  \n",
       "1     0   0                0          0             0  \n",
       "2     0   0                0          0             0  \n",
       "3     0   0                0          0             0  \n",
       "4     0   0                0          0             0  \n",
       "\n",
       "[5 rows x 30471 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')\n",
    "X = merged_df['merged_text']\n",
    "\n",
    "# convert to DataFrame\n",
    "X = cvec.fit_transform(X)\n",
    "text_df = pd.DataFrame(X.todense(), columns = cvec.get_feature_names())\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have some text in other languages where CountVectorizer doesn't return the expected result. This needs to be handled before we can move on to modeling. In order to take care of this, we will use a Google Translate library to translate any posts that aren't English into English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000000003</th>\n",
       "      <th>0000000001</th>\n",
       "      <th>0000006</th>\n",
       "      <th>000002322</th>\n",
       "      <th>000010</th>\n",
       "      <th>0004</th>\n",
       "      <th>000exp</th>\n",
       "      <th>...</th>\n",
       "      <th>что</th>\n",
       "      <th>шикарно</th>\n",
       "      <th>это</th>\n",
       "      <th>から毎週金曜よる6時55分にお引っ越しすることを記念し</th>\n",
       "      <th>そのうち10月6日</th>\n",
       "      <th>シャトウ</th>\n",
       "      <th>ツ_</th>\n",
       "      <th>テレヒ東京系にて放送中のアニメ</th>\n",
       "      <th>ホケットモンスター</th>\n",
       "      <th>特別な内容て開催されます</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000000003  0000000001  0000006  000002322  000010  0004  \\\n",
       "0   0    0     0            0           0        0          0       0     0   \n",
       "1   0    0     0            0           0        0          0       0     0   \n",
       "2   0    0     0            0           0        0          0       0     0   \n",
       "3   0    0     0            0           0        0          0       0     0   \n",
       "4   0    0     0            0           0        0          0       0     0   \n",
       "\n",
       "   000exp  ...  что  шикарно  это  から毎週金曜よる6時55分にお引っ越しすることを記念し  そのうち10月6日  \\\n",
       "0       0  ...    0        0    0                            0          0   \n",
       "1       0  ...    0        0    0                            0          0   \n",
       "2       0  ...    0        0    0                            0          0   \n",
       "3       0  ...    0        0    0                            0          0   \n",
       "4       0  ...    0        0    0                            0          0   \n",
       "\n",
       "   シャトウ  ツ_  テレヒ東京系にて放送中のアニメ  ホケットモンスター  特別な内容て開催されます  \n",
       "0     0   0                0          0             0  \n",
       "1     0   0                0          0             0  \n",
       "2     0   0                0          0             0  \n",
       "3     0   0                0          0             0  \n",
       "4     0   0                0          0             0  \n",
       "\n",
       "[5 rows x 30217 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thanks to this article for the following code: https://medium.com/analytics-vidhya/popular-python-libraries-in-nlp-dealing-with-language-detection-translation-beyond-7b8e7cb2928e\n",
    "\n",
    "# initialize Translator\n",
    "trans = Translator()\n",
    "texts = merged_df['merged_text'].copy()\n",
    "for i in range(len(texts)):\n",
    "    try:\n",
    "        texts.loc[i] = trans.translate(texts[i]).text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# CountVectorize\n",
    "cvec = CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')\n",
    "\n",
    "X = texts\n",
    "\n",
    "# convert to Dataframe\n",
    "X = cvec.fit_transform(X)\n",
    "text_df = pd.DataFrame(X.todense(), columns = cvec.get_feature_names())\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging steps to find strings that are only partially another language\n",
    "text_df[text_df['特別な内容て開催されます'] == 1]\n",
    "#texts.index[17182]\n",
    "#texts.loc[15225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the translation library only translates when the majority of text is another language, we need to do a few manually\n",
    "texts.loc[905] = trans.translate(texts[906], src = 'ru', dest = 'en').text\n",
    "texts.loc[1081] = trans.translate(texts[1081], src = 'ru', dest = 'en').text\n",
    "texts.loc[1203] = trans.translate(texts[1203], src = 'ru', dest = 'en').text\n",
    "texts.loc[6224] = trans.translate(texts[6270], src = 'ja', dest = 'en').text\n",
    "texts.loc[7880] = trans.translate(texts[7880], src = 'ja', dest = 'en').text\n",
    "texts.loc[8285] = 'Rank 9 player with &lt;2500 points Hi pvp guys, I think my GBL It\\'s buggy (Bugs in GBL is kinda new right?@_@)\\nToday i hit rank9 with 55% winrate, is practically the same as many others with a score of 2550+, but i only got 2200points which is too low for a rank9.\\nAt first I thought it would be something related to the \"hidden ELO\" so I continued to battle and managed to get a 1-4 and i didn\\'t lose points\\nSo I was wondering if this happened to anyone else and if there is anyone who can help me, maybe @NianticHelp'\n",
    "texts.loc[15119] = 'Is this a bug? https://i.imgur.com/e3sGsy9.png\\n\\n[Attraction with as [there] [were] six [hearts] Has anyone seen this before?'\n",
    "texts.loc[17061] = 'Kanto Cup so far, to the tune of Kung Fu Fighting https/imgur.com/4Rj6cCW\\n\\nIt literally only took me one set to get the screenshots after getting this stupid song stuck in my head. Powered up that Gust Pidgeot specifically after seeing every gorram lead be one of these two. Easy pickings, I guess'\n",
    "\n",
    "# countVectorize\n",
    "cvec = CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')\n",
    "\n",
    "X = texts\n",
    "\n",
    "X = cvec.fit_transform(X)\n",
    "text_df = pd.DataFrame(X.todense(), columns = cvec.get_feature_names())\n",
    "text_df.head()\n",
    "\n",
    "# save our translated dataframe so we don't have to keep waiting on translating\n",
    "merged_df['merged_text'] = texts\n",
    "merged_df.to_csv('../data/tranlated.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom preprocessing of text \n",
    "# thanks to the following article for the code: https://kavita-ganesan.com/how-to-use-countvectorizer/#CountVectorizer-Plain-and-Simple\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\W\", \" \", text) # remove special characters\n",
    "    text = re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \", text) # normalize certain words\n",
    "    \n",
    "    #stem words\n",
    "    words = re.split(\"\\\\s+\", text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\W\", \" \", text) # remove special characters\n",
    "    text = re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \", text) # normalize certain words\n",
    "    \n",
    "    #stem words\n",
    "    words = re.split(\"\\\\s+\", text)\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have resolved the issues with other languages, we can finally move onto creating a model to differentiate posts between TheSilphRoad and pokemongo subreddits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df = pd.read_csv('../data/translated.csv')\n",
    "# create a train_test_split\n",
    "X = translated_df['merged_text']\n",
    "y = translated_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines using count vectorizer or tfidf along with a classification model\n",
    "\n",
    "cvec_lr_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('logr', LogisticRegression(solver = 'liblinear', max_iter = 10000))\n",
    "])\n",
    "\n",
    "tfidf_lr_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('logr', LogisticRegression(solver = 'liblinear', max_iter = 10000))\n",
    "])\n",
    "\n",
    "cvec_nb_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "tfidf_nb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "cvec_knn_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "tfidf_knn_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "cvec_rforest_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('rforest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "tfidf_rforest_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('rforest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "cvec_xgb_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('xgb', XGBClassifier(use_label_encoder = False))\n",
    "])\n",
    "\n",
    "tfidf_xgb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('xgb', XGBClassifier(use_label_encoder = False))\n",
    "])\n",
    "\n",
    "cvec_svc_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "tfidf_svc_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "cvec_histboost_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('densify', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)), \n",
    "    ('histboost', HistGradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "tfidf_histboost_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "    ('densify', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "    ('histboost', HistGradientBoostingClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters for each pipeline\n",
    "\n",
    "cvec_lr_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor], \n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'logr__penalty': ['l1', 'l2'],\n",
    "    'logr__C': np.linspace(0.0001, 1, 1000)\n",
    "}\n",
    "\n",
    "tfidf_lr_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2),(1,3),(2,3), (2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'logr__penalty': ['l1', 'l2'],\n",
    "    'logr__C': np.linspace(0.0001, 1, 1000)\n",
    "}\n",
    "\n",
    "cvec_nb_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'nb__alpha': np.logspace(0, 6, 100)\n",
    "}\n",
    "\n",
    "tfidf_nb_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'nb__alpha': np.logspace(0, 6, 100)\n",
    "}\n",
    "\n",
    "cvec_knn_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'knn__n_neighbors': range(3, 100, 2)\n",
    "}\n",
    "\n",
    "tfidf_knn_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'knn__n_neighbors': range(3, 100, 2)\n",
    "}\n",
    "\n",
    "cvec_rforest_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'rforest__n_estimators': range(50, 1000, 50),\n",
    "    'rforest__max_depth': range(10, 101, 2),\n",
    "    'rforest__min_samples_split': range(2, 21, 2),\n",
    "    'rforest__min_samples_leaf': range(1, 50, 2), \n",
    "    'rforest__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "tfidf_rforest_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'rforest__n_estimators': range(50, 1000, 50),\n",
    "    'rforest__max_depth': range(10, 101, 2),\n",
    "    'rforest__min_samples_split': range(2, 21, 2),\n",
    "    'rforest__min_samples_leaf': range(1, 50, 2), \n",
    "    'rforest__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "cvec_xgb_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'xgb__n_estimators': range(10, 1000, 10), \n",
    "    'xgb__learning_rate': np.linspace(0.01, 1, 100),\n",
    "    'xgb__max_depth': range(10, 101, 2), \n",
    "    'xgb__min_child_weight': range(1, 1000, 100)\n",
    "}\n",
    "\n",
    "tfidf_xgb_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'xgb__n_estimators': range(10, 1000, 10), \n",
    "    'xgb__learning_rate': np.linspace(0.01, 1, 100),\n",
    "    'xgb__max_depth': range(10, 101, 2), \n",
    "    'xgb__min_child_weight': range(1, 1000, 100)\n",
    "}\n",
    "\n",
    "cvec_svc_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'svc__C': np.linspace(0.0001, 1, 100), \n",
    "    'svc__kernel': ['poly', 'rbf', 'linear'], \n",
    "    'svc__degree': range(2, 10)\n",
    "}\n",
    "\n",
    "tfidf_svc_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'svc__C': np.linspace(0.0001, 1, 100), \n",
    "    'svc__kernel': ['poly', 'rbf', 'linear'], \n",
    "    'svc__degree': range(2, 10)\n",
    "}\n",
    "\n",
    "cvec_histboost_params = {\n",
    "    'cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'cvec__min_df': range(1,4),\n",
    "    'cvec__max_features': range(2000, 100000, 100),\n",
    "    'histboost__loss': ['auto', 'binary_crossentropy'],\n",
    "    'histboost__learning_rate': np.linspace(0.01, 1, 100),\n",
    "    'histboost__max_iter': range(100, 10000, 100),\n",
    "    'histboost__max_leaf_nodes': range(2, 101, 2), \n",
    "    'histboost__max_depth': range(10, 101, 2),\n",
    "    'histboost__min_samples_leaf': range(1, 50, 2)\n",
    "}\n",
    "\n",
    "tfidf_histboost_params = {\n",
    "    'tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'tfidf__max_features': range(2000, 100000, 100),\n",
    "    'tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'tfidf__min_df': range(1,4),\n",
    "    'histboost__loss': ['auto', 'binary_crossentropy'],\n",
    "    'histboost__learning_rate': np.linspace(0.01, 1, 100),\n",
    "    'histboost__max_iter': range(100, 10000, 100),\n",
    "    'histboost__max_leaf_nodes': range(2, 101, 2), \n",
    "    'histboost__max_depth': range(10, 101, 2),\n",
    "    'histboost__min_samples_leaf': range(1, 50, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimator: Count Vectorizer Histogram Gradient Boost\n",
      "n_iterations: 7\n",
      "n_required_iterations: 8\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 20\n",
      "max_resources_: 15403\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 5000\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 5000 candidates, totalling 25000 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1667\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 1667 candidates, totalling 8335 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.45       0.45              nan ... 0.45       0.63333333 0.45      ]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.62333333 0.62333333        nan ... 0.62333333 1.         0.62333333]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 2\n",
      "n_candidates: 556\n",
      "n_resources: 180\n",
      "Fitting 5 folds for each of 556 candidates, totalling 2780 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.45 0.45  nan ...  nan  nan  nan]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.62333333 0.62333333        nan ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 3\n",
      "n_candidates: 186\n",
      "n_resources: 540\n",
      "Fitting 5 folds for each of 186 candidates, totalling 930 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.45       0.45              nan ... 0.56809524 0.56809524 0.56809524]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.62333333 0.62333333        nan ... 0.56482129 0.56901709 0.56482129]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 4\n",
      "n_candidates: 62\n",
      "n_resources: 1620\n",
      "Fitting 5 folds for each of 62 candidates, totalling 310 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.45       0.45              nan ... 0.58746971 0.62104534 0.56888197]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.62333333 0.62333333        nan ... 0.99814707 1.         0.99582689]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create gridsearches on our pipelines and parameters that have been setup\n",
    "cvec_lr_gs = HalvingRandomSearchCV(estimator = cvec_lr_pipe,\n",
    "                                param_distributions = cvec_lr_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_lr_gs = HalvingRandomSearchCV(estimator = tfidf_lr_pipe,\n",
    "                                param_distributions = tfidf_lr_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "cvec_nb_gs = HalvingRandomSearchCV(estimator = cvec_nb_pipe,\n",
    "                                param_distributions = cvec_nb_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_nb_gs = HalvingRandomSearchCV(estimator = tfidf_nb_pipe,\n",
    "                                param_distributions = tfidf_nb_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "cvec_knn_gs = HalvingRandomSearchCV(estimator = cvec_knn_pipe,\n",
    "                                param_distributions = cvec_knn_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_knn_gs = HalvingRandomSearchCV(estimator = tfidf_knn_pipe,\n",
    "                                param_distributions = tfidf_knn_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "cvec_rforest_gs = HalvingRandomSearchCV(estimator = cvec_rforest_pipe,\n",
    "                                param_distributions = cvec_rforest_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_rforest_gs = HalvingRandomSearchCV(estimator = tfidf_rforest_pipe,\n",
    "                                param_distributions = tfidf_rforest_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "cvec_xgb_gs = HalvingRandomSearchCV(estimator = cvec_xgb_pipe,\n",
    "                                param_distributions = cvec_xgb_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_xgb_gs = HalvingRandomSearchCV(estimator = tfidf_xgb_pipe,\n",
    "                                param_distributions = tfidf_xgb_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "cvec_svc_gs = HalvingRandomSearchCV(estimator = cvec_svc_pipe,\n",
    "                                param_distributions = cvec_svc_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_svc_gs = HalvingRandomSearchCV(estimator = tfidf_svc_pipe,\n",
    "                                param_distributions = tfidf_svc_params,\n",
    "                                n_candidates = 5000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "cvec_histboost_gs = HalvingRandomSearchCV(estimator = cvec_histboost_pipe,\n",
    "                                         param_distributions = cvec_histboost_params,\n",
    "                                         n_candidates = 5000,\n",
    "                                         verbose = 10, \n",
    "                                         cv = 5, \n",
    "                                         n_jobs = 50)\n",
    "\n",
    "tfidf_histboost_gs = HalvingRandomSearchCV(estimator = tfidf_histboost_pipe,\n",
    "                                          param_distributions = tfidf_histboost_params,\n",
    "                                          n_candidates = 5000,\n",
    "                                          verbose = 10,\n",
    "                                          cv = 5, \n",
    "                                          n_jobs = 50)\n",
    "\n",
    "# put all gridsearches into a list\n",
    "gridsearches = [cvec_lr_gs, tfidf_lr_gs, cvec_nb_gs, tfidf_nb_gs, cvec_knn_gs, tfidf_knn_gs, cvec_rforest_gs, \n",
    "                tfidf_rforest_gs, cvec_xgb_gs, tfidf_xgb_gs, cvec_svc_gs, tfidf_svc_gs, cvec_histboost_gs, tfidf_histboost_gs]\n",
    "\n",
    "# create dictionary of gridsearches\n",
    "gridsearch_dict = {0: 'Count Vectorizer Logistic Regression', 1: 'TFIDF Vectorizer Logistic Regression',\n",
    "                   2: 'Count Vectorizer Naive Bayes', 3: 'TFIDF Vectorizer Naive Bayes', \n",
    "                   4: 'Count Vectorizer K Nearest Neighbors', 5: 'TFIDF K Nearest Neighbors', \n",
    "                   6: 'Count Vectorizer Random Forest', 7: 'TFIDF Random Forest', \n",
    "                   8: 'Count Vectorizer XGBoost', 9: 'TFIDF Vectorizer XGBoost', \n",
    "                   10: 'Count Vectorizer Support Vector Machine', 11: 'TFIDF Vectorizer Support Vector Machine',\n",
    "                  12: 'Count Vectorizer Histogram Gradient Boost', 13: 'TFIDF Vectorizer Histogram Gradient Boost'}\n",
    "\n",
    "# thanks to the following article for the code below: https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-3.html\n",
    "\n",
    "best_acc = 0.0\n",
    "best_gs = ''\n",
    "best_model = 0\n",
    "\n",
    "for i, gs in enumerate(gridsearches):\n",
    "    print('\\nEstimator: %s' % gridsearch_dict[i])\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best Parameters: %s' % gs.best_params_)\n",
    "    print('Training accuracy: %.3f' % gs.best_score_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "    print('Test accurracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "    # get best model\n",
    "    if accuracy_score(y_test, y_pred) > best_acc:\n",
    "        best_acc = accuracy_score(y_test, y_pred)\n",
    "        best_gs = gs\n",
    "        best_model = i\n",
    "print('Model with best test accuracy: %s' % gridsearch_dict[best_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep track of our previous scores for our previous models in case we need to interrupt the loop:\n",
    "\n",
    "Estimator: Count Vectorizer Logistic Regression\n",
    "Best Parameters: {'logr__penalty': 'l2', 'logr__C': 0.04313873873873874, 'cvec__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 3, 'cvec__max_features': 53400, 'cvec__max_df': 0.8222222222222223}\n",
    "Training accuracy: 0.711\n",
    "Test accurracy: 0.721\n",
    "\n",
    "Estimator: TFIDF Logistic Regression\n",
    "Best Parameters: {'tfidf__preprocessor': <function stem_preprocessor at 0x7ff3584c41f0>, 'tfidf__ngram_range': (2, 3), 'tfidf__min_df': 3, 'tfidf__max_features': 40400, 'tfidf__max_df': 1.0, 'logr__penalty': 'l2', 'logr__C': 0.8578720720720721}\n",
    "Training accuracy: 0.667\n",
    "Test accurracy: 0.678\n",
    "\n",
    "Estimator: Count Vectorizer Naive Bayes\n",
    "Best Parameters: {'nb__alpha': 3.0538555088334154, 'cvec__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 1, 'cvec__max_features': 67100, 'cvec__max_df': 0.9555555555555555}\n",
    "Training accuracy: 0.690\n",
    "Test accurracy: 0.696\n",
    "\n",
    "Estimator: TFIDF Multinomial Naive Bayes\n",
    "Best Parameters: {'tfidf__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 3, 'tfidf__max_features': 72100, 'tfidf__max_df': 0.8, 'nb__alpha': 1.321941148466029}\n",
    "Training accuracy: 0.707\n",
    "Test accurracy: 0.720\n",
    "\n",
    "\n",
    "Estimator: Count Vectorizer K Nearest Neighbors\n",
    "Best Parameters: {'knn__n_neighbors': 59, 'cvec__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 3, 'cvec__max_features': 52900, 'cvec__max_df': 0.8}\n",
    "Training accuracy: 0.603\n",
    "Test accurracy: 0.596\n",
    "\n",
    "Estimator: TFIDF K Nearest Neighbors\n",
    "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
    "Best Parameters: {'tfidf__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 3, 'tfidf__max_features': 56500, 'tfidf__max_df': 0.8222222222222223, 'knn__n_neighbors': 71}\n",
    "Training accuracy: 0.681\n",
    "Test accurracy: 0.683\n",
    "\n",
    "Estimator: Count Vectorizer Random Forest\n",
    "Best Parameters: {'rforest__n_estimators': 500, 'rforest__min_samples_split': 10, 'rforest__min_samples_leaf': 1, 'rforest__max_features': 'sqrt', 'rforest__max_depth': 58, 'cvec__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 3, 'cvec__max_features': 36800, 'cvec__max_df': 0.9555555555555555}\n",
    "Training accuracy: 0.706\n",
    "Test accurracy: 0.722\n",
    "\n",
    "Estimator: TFIDF Random Forest\n",
    "Best Parameters: {'tfidf__preprocessor': <function stem_preprocessor at 0x7ff3584c41f0>, 'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 2, 'tfidf__max_features': 58100, 'tfidf__max_df': 1.0, 'rforest__n_estimators': 300, 'rforest__min_samples_split': 16, 'rforest__min_samples_leaf': 1, 'rforest__max_features': 'sqrt', 'rforest__max_depth': 94}\n",
    "Training accuracy: 0.711\n",
    "Test accurracy: 0.723\n",
    "\n",
    "Estimator: Count Vectorizer XGBoost\n",
    "Best Parameters: {'xgb__n_estimators': 440, 'xgb__min_child_weight': 1, 'xgb__max_depth': 10, 'xgb__learning_rate': 0.09999999999999999, 'cvec__preprocessor': <function stem_preprocessor at 0x7ff3584c41f0>, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 3, 'cvec__max_features': 8200, 'cvec__max_df': 0.9555555555555555}\n",
    "Training accuracy: 0.708\n",
    "Test accurracy: 0.712\n",
    "\n",
    "Estimator: TFIDF XGBoost\n",
    "Best Parameters: {'xgb__n_estimators': 90, 'xgb__min_child_weight': 1, 'xgb__max_depth': 54, 'xgb__learning_rate': 0.14, 'tfidf__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 17000, 'tfidf__max_df': 1.0}\n",
    "Training accuracy: 0.698\n",
    "Test accurracy: 0.703\n",
    "\n",
    "Estimator: Count Vectorizer Support Vector Machine\n",
    "Best Parameters: {'svc__kernel': 'linear', 'svc__degree': 9, 'svc__C': 0.1011, 'cvec__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'cvec__ngram_range': (3, 3), 'cvec__min_df': 3, 'cvec__max_features': 52400, 'cvec__max_df': 1.0}\n",
    "Training accuracy: 0.573\n",
    "Test accurracy: 0.586\n",
    "\n",
    "Estimator: TFIDF Support Vector Machine\n",
    "Best Parameters: {'tfidf__preprocessor': <function lemma_preprocessor at 0x7ff3584c4280>, 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 3, 'tfidf__max_features': 81700, 'tfidf__max_df': 0.9333333333333333, 'svc__kernel': 'rbf', 'svc__degree': 4, 'svc__C': 0.9798}\n",
    "Training accuracy: 0.715\n",
    "Test accurracy: 0.723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see that our best models had a test accuracy around 72%. While this is pretty good considering our subredits have people making posts about the same game, we would like to improve this to truly prove that TheSilphRoad subreddit is truly superior.   We will further tune this model in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with train and test scores of our models\n",
    "model_score = pd.DataFrame({'model': ['Baseline', 'Count Vectorizer Logistic Regression', 'TFIDF Logistic Regression',\n",
    "                                     'Count Vectorizer Naive Bayes', 'TFIDF Naive Bayes', \n",
    "                                     'Count Vectorizer K-Nearest Neighbors', 'TFIDF K-Nearest Neighbors', \n",
    "                                     'Count Vectorizer Random Forest', 'TFIDF Random Forest', \n",
    "                                     'Count Vectorizer XGBoost', 'TDIDF XGBoost', \n",
    "                                     'Count Vectorizer Support Vector Machine', 'TFIDF Support Vector Machine'] * 2,\n",
    "                           'train/test': ['train' if i <= 12 else 'test' for i in range(26)],\n",
    "                           'score': [0.55, 0.711, 0.667, 0.690, 0.707, 0.603, 0.681, 0.706, 0.711, 0.708, 0.698, 0.573, 0.715,\n",
    "                                    0.55, 0.721, 0.678, 0.696, 0.720, 0.596, 0.683, 0.722, 0.723, 0.712, 0.703, 0.586, 0.723]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar chart from our dataframe\n",
    "max_width = 10\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.title('Grid Search Accuracy Scores', fontsize = 30, fontweight = 'bold')\n",
    "ax = sns.barplot(y = model_score['score'], x = model_score['model'], hue = model_score['train/test'])\n",
    "ax.set_xticklabels(textwrap.fill(x.get_text(), max_width) for x in ax.get_xticklabels())\n",
    "ax.set_ylabel('score', fontsize = 20)\n",
    "ax.set_xlabel('model', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, many of our models perform very similarly. I was most surprised that the random forest models were not overfit as they are known to be high variance models, but I guess that is what happens when you grid search enough hyperparameters to remove the variance. Another surprise was that one of our simpler models performed better than XGBoost, which is unexpected as XGBoost is a famous model for winning many classification competitions in the past. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
