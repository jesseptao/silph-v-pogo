{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer: Gridsearches are run using 50 CPU threads, so don't run unless you want to wait eons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will evalute our baseline model and attempt to build a better model using sentiment analysis and a voting classifier using our previous models as part of the voting classifier if we have time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting, enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \\\n",
    "VotingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>merged_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fix to not being able to attack? Has anybody f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Attack glitch during Regi raids 2 raids today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Bug?] Can’t seem to earn or collect pokecoins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[Bug?] AR suddenly freezes Using an iPhone 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3 hour incense event personal results For any ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                        merged_text\n",
       "0          1  Fix to not being able to attack? Has anybody f...\n",
       "1          1  Attack glitch during Regi raids 2 raids today ...\n",
       "2          1  [Bug?] Can’t seem to earn or collect pokecoins...\n",
       "3          1  [Bug?] AR suddenly freezes Using an iPhone 11,...\n",
       "4          1  3 hour incense event personal results For any ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "\n",
    "translated_df = pd.read_csv('../data/translated.csv')\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries where text is the same\n",
    "translated_df = translated_df.drop_duplicates('merged_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get get sentiment scores for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks to Hov\n",
    "# instantiate the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def get_compound_sentiment(post):\n",
    "    return sia.polarity_scores(post)['compound']\n",
    "\n",
    "translated_df['sentiment'] = translated_df['merged_text'].apply(get_compound_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(lambda x: x['merged_text'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['sentiment']], validate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10611\n",
       "0     8642\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create baseline model\n",
    "translated_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will guess that all posts are from TheSilphRoad subreddit as it is our majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train_test_split to evaluate our baseline model\n",
    "X = translated_df[['merged_text', 'sentiment']]\n",
    "y = translated_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on training data: 0.55\n",
      "Baseline accuracy on testing data: 0.55\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "\n",
    "# create predictions for train and test set\n",
    "y_train_preds = np.full_like(y_train, 1)\n",
    "y_test_preds = np.full_like(y_test, 1)\n",
    "\n",
    "print(f'Baseline accuracy on training data: {round(accuracy_score(y_train, y_train_preds), 2)}')\n",
    "print(f'Baseline accuracy on testing data: {round(accuracy_score(y_test, y_test_preds), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have an baseline accuracy of 55% on both our training and testing data which is expected as about 55% of the posts we have are from TheSilphRoad subreddit and we used stratify for our train/test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom stop words list to remove all similar words we found in the previous notebook\n",
    "\n",
    "# start with the base english stopwords\n",
    "new_stopwords = stopwords.words('english')\n",
    "\n",
    "# add stopwords that will easily identify a silph post\n",
    "# also added stopwords that are common across both subreddits and stop words as a result and lemmatizing and stemming\n",
    "custom_words = ['silph', 'road', 'silphroad', 'thesilphroad', 'pokemon', 'go', 'get', 'one', 'like', 'would', 'know', 'time', 'game', 'shiny', \n",
    "               'https', 'raid', 'anyone', 'got', 'new', 'event', 'day', 'level', 'even', 'com', 'raids', 'still', 'people', 'also', 'since',\n",
    "               'use', 'catch', 'amp', 'see', 'want', 'could', 'first', 'research', 'shadow', 'think', 'else', 'way', 'niantic', 'make', \n",
    "               'back', 'really', 'need', 'eggs', 'community', 'something', 'much', 'good', 'able', \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'abl', \n",
    "                'abov', 'ani', 'anyon', 'becaus', 'befor', 'commun', 'doe', 'dure', 'egg', 'els', 'ha', 'hi', 'http', 'might', 'must', \n",
    "                \"n't\", 'onc', 'onli', 'ourselv', 'peopl', 'realli', 'sha', 'shini', 'sinc', 'someth', 'themselv', 'thi', 'veri', 'wa', \n",
    "                'whi', 'wo', 'yourselv', 'becau', 'el']\n",
    "\n",
    "new_stopwords.extend(custom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom preprocessing of text \n",
    "# thanks to the following article for the code: https://kavita-ganesan.com/how-to-use-countvectorizer/#CountVectorizer-Plain-and-Simple\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\W\", \" \", text) # remove special characters\n",
    "    text = re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \", text) # normalize certain words\n",
    "    \n",
    "    #stem words\n",
    "    words = re.split(\"\\\\s+\", text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\W\", \" \", text) # remove special characters\n",
    "    text = re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \", text) # normalize certain words\n",
    "    \n",
    "    #stem words\n",
    "    words = re.split(\"\\\\s+\", text)\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate our pipelines\n",
    "cvec_sentiment_pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer(stop_words = new_stopwords, strip_accents = 'unicode'))\n",
    "            ]))\n",
    "    ])),\n",
    "    ('densify', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "    ('vote', VotingClassifier([\n",
    "        ('logr', LogisticRegression(solver = 'liblinear', max_iter = 10000)),\n",
    "        ('rforest', RandomForestClassifier()),\n",
    "        ('bag', BaggingClassifier()),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('etree', ExtraTreesClassifier()),\n",
    "        ('xgb', XGBClassifier(use_label_encoder = False, n_jobs = 1, eval_metric = \"logloss\")),\n",
    "        ('svc', SVC()), \n",
    "        ('histboost', HistGradientBoostingClassifier())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "tfidf_sentiment_pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('tfidf', TfidfVectorizer(stop_words = new_stopwords, strip_accents = 'unicode')),\n",
    "            ]))\n",
    "    ])),\n",
    "    ('densify', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "    ('vote', VotingClassifier([\n",
    "        ('logr', LogisticRegression(solver = 'liblinear', max_iter = 10000)),\n",
    "        ('rforest', RandomForestClassifier()),\n",
    "        ('bag', BaggingClassifier()),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('etree', ExtraTreesClassifier()),\n",
    "        ('xgb', XGBClassifier(use_label_encoder = False, n_jobs = 1, eval_metric = \"logloss\")),\n",
    "        ('svc', SVC()),\n",
    "        ('histboost', HistGradientBoostingClassifier())\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gridsearch parameters\n",
    "\n",
    "cvec_sentiment_params = {\n",
    "    'features__text_features__cvec__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'features__text_features__cvec__ngram_range': [(1,1), (1,2), (1,3), (2,3),(3,3)],\n",
    "    'features__text_features__cvec__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'features__text_features__cvec__min_df': range(1,4),\n",
    "    'features__text_features__cvec__max_features': range(10000, 100000, 100),\n",
    "    'vote__logr__penalty': ['l1', 'l2'],\n",
    "    'vote__logr__C': np.linspace(0.0001, 1, 50),\n",
    "    'vote__rforest__n_estimators': range(50, 1000, 25),\n",
    "    'vote__rforest__max_depth': range(10, 101, 4),\n",
    "    'vote__rforest__max_features': ['sqrt', 'log2'],\n",
    "    'vote__xgb__n_estimators': range(10, 1000, 100), \n",
    "    'vote__xgb__learning_rate': [0.01, 0.1, 0.3, 0.7],\n",
    "    'vote__xgb__max_depth': [2, 5, 10], \n",
    "    'vote__xgb__min_child_weight': range(2, 100, 4), \n",
    "    'vote__histboost__learning_rate': np.linspace(0.01, 1, 50),\n",
    "    'vote__histboost__max_iter': range(100, 10000, 200),\n",
    "    'vote__histboost__max_depth': range(10, 101, 4),\n",
    "}\n",
    "\n",
    "tfidf_sentiment_params = {\n",
    "    'features__text_features__tfidf__preprocessor': [lemma_preprocessor, stem_preprocessor],\n",
    "    'features__text_features__tfidf__ngram_range': [(1,1), (1,2), (1,3), (2,3),(2,2),(3,3)],\n",
    "    'features__text_features__tfidf__max_df': np.linspace(0.8, 1.0, 10),\n",
    "    'features__text_features__tfidf__min_df': range(1,4),\n",
    "    'features__text_features__tfidf__max_features': range(2000, 100000, 100),\n",
    "    'vote__logr__penalty': ['l1', 'l2'],\n",
    "    'vote__logr__C': np.linspace(0.0001, 1, 1000),\n",
    "    'vote__rforest__n_estimators': range(50, 1000, 50),\n",
    "    'vote__rforest__max_depth': range(10, 101, 2),\n",
    "    'vote__rforest__max_features': ['sqrt', 'log2'],\n",
    "    'vote__xgb__n_estimators': range(10, 1000, 50), \n",
    "    'vote__xgb__learning_rate': [0.01, 0.1, 0.3, 0.7],\n",
    "    'vote__xgb__max_depth': [2, 5, 10], \n",
    "    'vote__xgb__min_child_weight': range(2, 100, 2), \n",
    "    'vote__histboost__learning_rate': np.linspace(0.01, 1, 100),\n",
    "    'vote__histboost__max_iter': range(100, 10000, 100),\n",
    "    'vote__histboost__max_depth': range(10, 101, 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_sentiment_gs = HalvingRandomSearchCV(estimator = cvec_sentiment_pipe,\n",
    "                                param_distributions = cvec_sentiment_params,\n",
    "                                n_candidates = 10000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)\n",
    "\n",
    "tfidf_sentiment_gs = HalvingRandomSearchCV(estimator = tfidf_sentiment_pipe,\n",
    "                                param_distributions = tfidf_sentiment_params,\n",
    "                                n_candidates = 10000,\n",
    "                                verbose = 10, \n",
    "                                cv = 5, \n",
    "                                n_jobs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimator: Count Vectorizer Sentiment Voting\n",
      "n_iterations: 7\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 20\n",
      "max_resources_: 15402\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 10000\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 10000 candidates, totalling 50000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75       0.55       0.61666667 ...        nan 0.48333333 0.48333333]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.95   0.9125 0.8875 ...    nan 0.8875 0.925 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3334\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 3334 candidates, totalling 16670 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75       0.55       0.61666667 ... 0.43787879 0.49090909        nan]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.95       0.9125     0.8875     ... 0.80230496 0.80691489        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 2\n",
      "n_candidates: 1112\n",
      "n_resources: 180\n",
      "Fitting 5 folds for each of 1112 candidates, totalling 5560 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 371\n",
      "n_resources: 540\n",
      "Fitting 5 folds for each of 371 candidates, totalling 1855 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75       0.55       0.61666667 ... 0.54714286 0.5468254  0.5531746 ]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.95       0.9125     0.8875     ... 0.84950466 0.88021562 0.85930458]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 4\n",
      "n_candidates: 124\n",
      "n_resources: 1620\n",
      "Fitting 5 folds for each of 124 candidates, totalling 620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75       0.55       0.61666667 ... 0.58459675 0.56036691 0.56780893]\n",
      "  warnings.warn(\n",
      "/home/jesse/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [0.95       0.9125     0.8875     ... 0.95411833 0.97683144 0.95644603]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# put all gridsearches into a list\n",
    "gridsearches = [cvec_sentiment_gs, tfidf_sentiment_gs]\n",
    "\n",
    "# create dictionary of gridsearches\n",
    "gridsearch_dict = {0: 'Count Vectorizer Sentiment Voting', 1: 'TFIDF Sentiment Voting'}\n",
    "\n",
    "# thanks to the following article for the code below: https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-3.html\n",
    "\n",
    "for i, gs in enumerate(gridsearches):\n",
    "    print('\\nEstimator: %s' % gridsearch_dict[i])\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best Parameters: %s' % gs.best_params_)\n",
    "    print('Training accuracy: %.3f' % gs.best_score_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "    print('Test accurracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still waiting on this extensive gridsearch to finish and may not be finished by the deadline to turn this in. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
